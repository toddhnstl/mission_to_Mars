{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scrape everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary will hold everything we pull from all the sites\n",
    "scraped_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site 1 - \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "news_url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\" # probably need to replace this since it redirects\n",
    "news_response = requests.get(news_url)\n",
    "time.sleep(2)\n",
    "\n",
    "# use beautiful soup to parse the url above\n",
    "news_soup = bs(news_response.text, 'html.parser')\n",
    "#newPSoup = bs(news_response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example_title_div = '<div class=\"content_title\"><a href=\"/news/8520/nasas-mars-2020-rover-tests-descent-stage-separation/\" target=\"_self\">NASAs Mars 2020 Rover Tests Descent-Stage Separation</a></div>'\n",
    "# # example_paragraph_div = '<div class=\"article_teaser_body\">A crane lifts the rocket-powered descent stage away from NASAs Mars 2020 rover after technicians tested the pyrotechnic charges that separate the two spacecraft.</div>'\n",
    "\n",
    "\n",
    "# # use bs to find() the example_title_div and filter on the class_='content_tile'\n",
    "\n",
    "\n",
    "# # presults = newPSoup.find_all('li', class_='slide')\n",
    "# # print(presults)\n",
    "\n",
    "# results = news_soup.find('div', class_='content_title')\n",
    "# print(\"==== News Title =====\")\n",
    "# print(results)\n",
    "# print(\"\")\n",
    "# news_title = results.a.text\n",
    "# news_title = news_title.replace('\\n', '')\n",
    "# print(news_title)\n",
    "# scraped_data['news_title'] = news_title #- load the dataframe with Key/value pair\n",
    "# print(\"=========\")\n",
    "# print(\"\")\n",
    "# print(\"\")\n",
    "\n",
    "# # use bs to find() the example_title_div and filter on the class_='article_teaser_body'\n",
    "# presults = news_soup.find('div', class_='article_teaser_body')\n",
    "# print(\"====Paragraph=====\")\n",
    "# print(presults)\n",
    "# print(\"=========\")\n",
    "# print(\"\")\n",
    "# print(\"\")\n",
    "\n",
    "# # # news_p = \"FILL IN THE PARAGRAPH\"\n",
    "# # # scraped_data['news_p'] = news_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'm getting nowhere with just BS.  Try site 1 with splinter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"New evidence suggests salty, shallow ponds once dotted a Martian crater â€” a sign of the planet's drying climate.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "browser.visit(news_url)\n",
    "time.sleep(5)\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "# Get an article block.\n",
    "art_teaser_body = soup.find('li', class_='slide')\n",
    "\n",
    "# Parse out the title\n",
    "title = art_teaser_body.find('div',  class_='content_title')\n",
    "news_title = title.text\n",
    "\n",
    "# Parse out the body text\n",
    "news_p = art_teaser_body.find('div',  class_='article_teaser_body').text\n",
    "news_p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data['news_title'] = news_title\n",
    "scraped_data['news_p'] = news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jpl.nasa.gov/spaceimages/images/wallpaper/PIA19968-1920x1200.jpg\n",
      "Charon in Enhanced Color\n"
     ]
    }
   ],
   "source": [
    "# site 2 - https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\n",
    "base_url = 'https://www.jpl.nasa.gov'\n",
    "# # use splinter to connect to the url and navigate, then use bs4 to repeat what you did in site 1\n",
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "# browser.visit(url)\n",
    "\n",
    "image_response = requests.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# use beautiful soup to parse the url above\n",
    "img_soup = bs(image_response.text, 'html.parser')\n",
    "time.sleep(5)\n",
    "img = img_soup.find('article', class_='carousel_item')\n",
    "str_img = img['style']\n",
    "a, featured_image_url, c = str_img.split(\"'\")\n",
    "\n",
    "\n",
    "featured_image_url = base_url + featured_image_url\n",
    "print(featured_image_url)\n",
    "\n",
    "featured_image_caption = img_soup.find('h1', class_='media_feature_title').text\n",
    "featured_image_caption = featured_image_caption.strip()\n",
    "print(featured_image_caption)\n",
    "\n",
    "\n",
    "# Load it to the dataframe\n",
    "scraped_data['featured_image_url'] = featured_image_url\n",
    "scraped_data['featured_image_caption'] = featured_image_caption\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mystr = \"background-image: url('/spaceimages/images/wallpaper/PIA18886-1920x1200.jpg');\"\n",
    "# a, b, c = mystr.split('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site 3 - https://twitter.com/marswxreport?lang=en\n",
    "\n",
    "# grab the latest tweet and be careful its a weather tweet\n",
    "# P class='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'\n",
    "# Example:\n",
    "#mars_weather = 'Sol 1801 (Aug 30, 2017), Sunny, high -21C/-5F, low -80C/-112F, pressure at 8.82 hPa, daylight 06:09-17:55'\n",
    "# These seem to always start with 'InSight Sol'\n",
    "time.sleep(5)\n",
    "\n",
    "twit_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "\n",
    "browser.visit(twit_url)\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "time.sleep(5)\n",
    "\n",
    "# Get an article block.\n",
    "weather_block = soup.find_all('div', class_='js-tweet-text-container')\n",
    "# print(\"Printing Weather Block\")\n",
    "# print(weather_block)\n",
    "\n",
    "\n",
    "# print(\"starting for tweet loop\")\n",
    "for tweet in weather_block:\n",
    "#     print(\"---  Begin inside loop ----\")\n",
    "    p_text = tweet.p.text\n",
    "#     print(p_text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # do a regex on the string to see if there is 'InSight Sol'\n",
    "    # at beginning of string.  \n",
    "    #  result = re.match(pattern, string)\n",
    "    cmpld_str = re.compile('InSight')\n",
    "    \n",
    "    result = cmpld_str.match(p_text)\n",
    "#     print(result)\n",
    "#     print(result.group())\n",
    "    if result.group() == 'InSight':\n",
    "#         print(\"Inside if result for RegEx\")\n",
    "#         print(p_text)\n",
    "        \n",
    "        # Store it in the dataframe\n",
    "        scraped_data['weather_data'] = p_text\n",
    "#         print(\"\")\n",
    "        break\n",
    "    # Once we confirm we found a weather post with InSight, exit loop.\n",
    "    \n",
    "#     print(\"---  End inside loop ----\")\n",
    "#     print(\"\")\n",
    "    \n",
    "# print(\"End of tweet loop\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site 4 - \n",
    "facts_url = 'https://space-facts.com/mars/'\n",
    "\n",
    "# use pandas to parse the table\n",
    "\n",
    "facts_df = pd.read_html(facts_url)[0]\n",
    "# print(facts_df)\n",
    "\n",
    "\n",
    "# convert facts_df to a html string and add to dictionary.\n",
    "facts_html_str = facts_df.to_html('templates/facts_table.html', index=False, justify='center', table_id='facts_table')\n",
    "# print(facts_html_str)\n",
    "\n",
    "scraped_data['facts_html_str'] = facts_html_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # site 5 \n",
    "\n",
    "# # set base URL for this site\n",
    "# astro_base_url = 'https://astrogeology.usgs.gov'\n",
    "\n",
    "# # use bs4 to scrape the title and url and add to dictionary\n",
    "# hemi_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "# browser.visit(hemi_url)\n",
    "# time.sleep(5)\n",
    "# html = browser.html\n",
    "# soup = bs(html, 'html.parser')\n",
    "\n",
    "\n",
    "# #- get all the listings.\n",
    "# hemi_block = soup.find_all('div', class_='description')\n",
    "\n",
    "\n",
    "# ll_hemi_image_urls = []\n",
    "# cntr = 0\n",
    "# for block in hemi_block:\n",
    "#     cntr +=1\n",
    "# #     print(f'Counter is: {cntr}')\n",
    "#     img_url = block.a['href'] #- this isn't valid yet. have to nav to it then get full link to image\n",
    "#     title = block.a.text\n",
    "# #     print(f'Url to image page: {img_url}')\n",
    "# #     print(f'title is: {title}')  #- this is valid.\n",
    "\n",
    "#     full_img_url = astro_base_url + img_url\n",
    "#     browser.visit(full_img_url)\n",
    "#     time.sleep(5)\n",
    "#     iHtml = browser.html\n",
    "#     isoup = bs(iHtml, 'html.parser')\n",
    "#     tmp = isoup.find('div', class_='downloads')\n",
    "# #     print(f'tmp is now: {tmp}')\n",
    "    \n",
    "#     img_url = tmp.a['href']\n",
    "# #     print(f'img_url is now: {img_url}')\n",
    "# #     print('==============')\n",
    "# #     print('')\n",
    "#     ll_hemi_image_urls.append({\"title\": title, \"img_url\": img_url})\n",
    "    \n",
    "# # print(ll_hemi_image_urls) \n",
    "# scraped_data['hemisphere_image_urls'] = ll_hemi_image_urls\n",
    "\n",
    "# # # Example:\n",
    "# # hemisphere_image_urls = [\n",
    "# #     {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"...\"},\n",
    "# #     {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"...\"},\n",
    "# #     {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"...\"},\n",
    "# #     {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"...\"},\n",
    "# # ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'news_title': \"NASA's Curiosity Rover Finds an Ancient Oasis on Mars\", 'news_p': \"New evidence suggests salty, shallow ponds once dotted a Martian crater â€” a sign of the planet's drying climate.\", 'featured_image_url': 'https://www.jpl.nasa.gov/spaceimages/images/wallpaper/PIA19968-1920x1200.jpg', 'featured_image_caption': 'Charon in Enhanced Color', 'weather_data': 'InSight sol 309 (2019-10-10) low -102.3ÂºC (-152.1ÂºF) high -26.2ÂºC (-15.1ÂºF)\\nwinds from the SSE at 6.1 m/s (13.6 mph) gusting to 18.9 m/s (42.4 mph)\\npressure at 7.20 hPapic.twitter.com/sSOjseIl81', 'facts_html_str': None}\n"
     ]
    }
   ],
   "source": [
    "# Close the browser\n",
    "browser.quit()\n",
    "\n",
    "# Print the data gather out \n",
    "print(scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File-> download as python into a new module called scrape_mars.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use day 3 09-Ins_Scrape_And_Render/app.py as a blue print on how to finish the homework.\n",
    "\n",
    "# replace the contents of def index() and def scraper() appropriately.\n",
    "\n",
    "# change the index.html to render the site with all the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pandas] *",
   "language": "python",
   "name": "conda-env-pandas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
