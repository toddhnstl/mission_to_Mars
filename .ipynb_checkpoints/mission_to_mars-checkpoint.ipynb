{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scrape everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary will hold everything we pull from all the sites\n",
    "scraped_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site 1 - \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "news_url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\" # probably need to replace this since it redirects\n",
    "news_response = requests.get(news_url)\n",
    "time.sleep(2)\n",
    "\n",
    "# use beautiful soup to parse the url above\n",
    "news_soup = bs(news_response.text, 'html.parser')\n",
    "#newPSoup = bs(news_response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example_title_div = '<div class=\"content_title\"><a href=\"/news/8520/nasas-mars-2020-rover-tests-descent-stage-separation/\" target=\"_self\">NASAs Mars 2020 Rover Tests Descent-Stage Separation</a></div>'\n",
    "# # example_paragraph_div = '<div class=\"article_teaser_body\">A crane lifts the rocket-powered descent stage away from NASAs Mars 2020 rover after technicians tested the pyrotechnic charges that separate the two spacecraft.</div>'\n",
    "\n",
    "\n",
    "# # use bs to find() the example_title_div and filter on the class_='content_tile'\n",
    "\n",
    "\n",
    "# # presults = newPSoup.find_all('li', class_='slide')\n",
    "# # print(presults)\n",
    "\n",
    "# results = news_soup.find('div', class_='content_title')\n",
    "# print(\"==== News Title =====\")\n",
    "# print(results)\n",
    "# print(\"\")\n",
    "# news_title = results.a.text\n",
    "# news_title = news_title.replace('\\n', '')\n",
    "# print(news_title)\n",
    "# scraped_data['news_title'] = news_title #- load the dataframe with Key/value pair\n",
    "# print(\"=========\")\n",
    "# print(\"\")\n",
    "# print(\"\")\n",
    "\n",
    "# # use bs to find() the example_title_div and filter on the class_='article_teaser_body'\n",
    "# presults = news_soup.find('div', class_='article_teaser_body')\n",
    "# print(\"====Paragraph=====\")\n",
    "# print(presults)\n",
    "# print(\"=========\")\n",
    "# print(\"\")\n",
    "# print(\"\")\n",
    "\n",
    "# # # news_p = \"FILL IN THE PARAGRAPH\"\n",
    "# # # scraped_data['news_p'] = news_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'm getting nowhere with just BS.  Try site 1 with splinter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit(news_url)\n",
    "time.sleep(5)\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "# Get an article block.\n",
    "art_teaser_body = soup.find('li', class_='slide')\n",
    "\n",
    "# Parse out the title\n",
    "title = art_teaser_body.find('div',  class_='content_title')\n",
    "news_title = title.text\n",
    "\n",
    "# Parse out the body text\n",
    "news_p = art_teaser_body.find('div',  class_='article_teaser_body').text\n",
    "news_p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data['news_title'] = news_title\n",
    "scraped_data['news_p'] = news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site 2 - https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\n",
    "base_url = 'https://www.jpl.nasa.gov'\n",
    "# use splinter to connect to the url and navigate, then use bs4 to repeat what you did in site 1\n",
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(url)\n",
    "time.sleep(5)\n",
    "\n",
    "browser.click_link_by_partial_text('FULL IMAGE')\n",
    "time.sleep(5)\n",
    "\n",
    "# object img  class=fancybox-image\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "img_class = soup.find('img', class_='fancybox-image')\n",
    "img_url = img_class['src']\n",
    "featured_image_url = base_url + img_url\n",
    "# print(featured_image_url)\n",
    "\n",
    "# Load it to the dataframe\n",
    "scraped_data['featured_image_url'] = featured_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site 3 - https://twitter.com/marswxreport?lang=en\n",
    "\n",
    "# grab the latest tweet and be careful its a weather tweet\n",
    "# P class='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'\n",
    "# Example:\n",
    "#mars_weather = 'Sol 1801 (Aug 30, 2017), Sunny, high -21C/-5F, low -80C/-112F, pressure at 8.82 hPa, daylight 06:09-17:55'\n",
    "# These seem to always start with 'InSight Sol'\n",
    "time.sleep(5)\n",
    "\n",
    "twit_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "\n",
    "browser.visit(twit_url)\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "time.sleep(5)\n",
    "\n",
    "# Get an article block.\n",
    "weather_block = soup.find_all('div', class_='js-tweet-text-container')\n",
    "# print(\"Printing Weather Block\")\n",
    "# print(weather_block)\n",
    "\n",
    "\n",
    "# print(\"starting for tweet loop\")\n",
    "for tweet in weather_block:\n",
    "#     print(\"---  Begin inside loop ----\")\n",
    "    p_text = tweet.p.text\n",
    "#     print(p_text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # do a regex on the string to see if there is 'InSight Sol'\n",
    "    # at beginning of string.  \n",
    "    #  result = re.match(pattern, string)\n",
    "    cmpld_str = re.compile('InSight')\n",
    "    \n",
    "    result = cmpld_str.match(p_text)\n",
    "#     print(result)\n",
    "#     print(result.group())\n",
    "    if result.group() == 'InSight':\n",
    "#         print(\"Inside if result for RegEx\")\n",
    "#         print(p_text)\n",
    "        \n",
    "        # Store it in the dataframe\n",
    "        scraped_data['weather_data'] = p_text\n",
    "#         print(\"\")\n",
    "        break\n",
    "    # Once we confirm we found a weather post with InSight, exit loop.\n",
    "    \n",
    "#     print(\"---  End inside loop ----\")\n",
    "#     print(\"\")\n",
    "    \n",
    "# print(\"End of tweet loop\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site 4 - \n",
    "facts_url = 'https://space-facts.com/mars/'\n",
    "\n",
    "# use pandas to parse the table\n",
    "\n",
    "facts_df = pd.read_html(facts_url)[0]\n",
    "# print(facts_df)\n",
    "\n",
    "\n",
    "# convert facts_df to a html string and add to dictionary.\n",
    "facts_html_str = facts_df.to_html(index=False, justify='center', table_id='facts_table')\n",
    "# print(facts_html_str)\n",
    "\n",
    "scraped_data['facts_html_str'] = facts_html_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site 5 \n",
    "\n",
    "# set base URL for this site\n",
    "astro_base_url = 'https://astrogeology.usgs.gov'\n",
    "\n",
    "# use bs4 to scrape the title and url and add to dictionary\n",
    "hemi_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(hemi_url)\n",
    "time.sleep(5)\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "\n",
    "#- get all the listings.\n",
    "hemi_block = soup.find_all('div', class_='description')\n",
    "\n",
    "\n",
    "ll_hemi_image_urls = []\n",
    "cntr = 0\n",
    "for block in hemi_block:\n",
    "    cntr +=1\n",
    "#     print(f'Counter is: {cntr}')\n",
    "    img_url = block.a['href'] #- this isn't valid yet. have to nav to it then get full link to image\n",
    "    title = block.a.text\n",
    "#     print(f'Url to image page: {img_url}')\n",
    "#     print(f'title is: {title}')  #- this is valid.\n",
    "\n",
    "    full_img_url = astro_base_url + img_url\n",
    "    browser.visit(full_img_url)\n",
    "    time.sleep(5)\n",
    "    iHtml = browser.html\n",
    "    isoup = bs(iHtml, 'html.parser')\n",
    "    tmp = isoup.find('div', class_='downloads')\n",
    "#     print(f'tmp is now: {tmp}')\n",
    "    \n",
    "    img_url = tmp.a['href']\n",
    "#     print(f'img_url is now: {img_url}')\n",
    "#     print('==============')\n",
    "#     print('')\n",
    "    ll_hemi_image_urls.append({\"title\": title, \"img_url\": img_url})\n",
    "    \n",
    "# print(ll_hemi_image_urls) \n",
    "scraped_data['hemisphere_image_urls'] = ll_hemi_image_urls\n",
    "\n",
    "# # Example:\n",
    "# hemisphere_image_urls = [\n",
    "#     {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"...\"},\n",
    "#     {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"...\"},\n",
    "#     {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"...\"},\n",
    "#     {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"...\"},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "browser.quit()\n",
    "\n",
    "# Print the data gather out \n",
    "print(scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File-> download as python into a new module called scrape_mars.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use day 3 09-Ins_Scrape_And_Render/app.py as a blue print on how to finish the homework.\n",
    "\n",
    "# replace the contents of def index() and def scraper() appropriately.\n",
    "\n",
    "# change the index.html to render the site with all the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pandas] *",
   "language": "python",
   "name": "conda-env-pandas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
